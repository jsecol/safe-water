---
title: "Get spatial data for EPA region 1"
date: "`r Sys.Date()`"
output:
  github_document:
    toc: true
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 0 - Load packages  

```{r message=FALSE}

# library(RMySQL)
# library(data.table)
library(googledrive)
library(fs)
library(readr)
library(RPostgreSQL)
library(dplyr)
library(ggplot2)
library(maps) # has some basic vector basemaps
# library(zipcode) # for getting lat/lon coordinates where zipcode is available
# library(lubridate)
# library(noncensus) holding off for now, try ZCTA directly, can match to US Census data at that resolution

```

## I - Load data

1. Get available files from my google drive (public)

```{r}

files <- drive_find(type = "csv", q = c("visibility = 'anyoneCanFind'"))

files$name

```

2. Download into R

```{r}

dir_create("data_temp")

drive_download(file = files[1,], 
               path = paste0("data_temp/", files[1,]$name),
               overwrite = TRUE)

```

3. Read back in

```{r}

locations <- read_csv(paste0("data_temp/", files[1,]$name))

```

4. Get other data

```{r}

#### My Postgres
drv <- dbDriver("PostgreSQL")

con <- dbConnect(drv, dbname = "postgres",
                 host = "localhost", port = 5432,
                 user = "postgres")

dbListTables(con)

```

+ Query Active R01 PWS from WATER_SYSTEM & Violations

```{r}

water_systems <- 
  dbGetQuery(con, "SELECT * FROM sdwis.water_system WHERE pws_activity_code = 'A' 
             AND epa_region = '01'")

violations <- 
  dbGetQuery(con, "SELECT * FROM sdwis.violation WHERE pws_activity_code = 'A' 
             AND epa_region = '01'")

dbDisconnect(con)

```

+ ECHO Maybe useful later

```{r}
# df_echo <- dbGetQuery(con, "SELECT * FROM echo.echo_exporter WHERE sdwis_flag = 'Y'")
# LIMIT 10 

```



## II - Process data (filter and join)

1. focus on one contaminent





## III - Get environmental data (worldclim - try to get just NE at highest resolution)





















#### Starting with facs having a fac_zip, presumably is in reasonably correct location


```{r}
merge1 <- df_echo %>% filter(!is.na(fac_zip)) %>% 
  select(sdwa_ids, fac_zip, fac_lat, fac_long, fac_city, fac_county, fac_state,
         fac_collection_method, fac_accuracy_meters) %>% 
  mutate(echo_dat = "fac_zip used") %>% 
  right_join(., water_system, by = c("sdwa_ids" = "PWSID")) %>% 
  rename(PWSID = sdwa_ids)

merge1 %>% filter(!is.na(fac_zip)) %>% 
  count() %>% .$n

merge1 %>% filter(!is.na(fac_zip), PWS_TYPE_CODE == "CWS") %>% 
  count() %>% .$n

# maybe not perfect (e.g., NA, County Centroid), but OK overall hopefully
merge1 %>% filter(!is.na(fac_zip), PWS_TYPE_CODE == "CWS") %>% 
  group_by(fac_collection_method) %>% 
  count() %>% arrange(-n)

```

<br>

#### Seems best to use fac_zip when available

+ make some type changes, explore

```{r}
merge1 <- merge1 %>% mutate(fac_lat = as.numeric(fac_lat), 
                            fac_long =  as.numeric(fac_long),
                            fac_accuracy_meters = as.numeric(fac_accuracy_meters))

# some AK, Island Territories, etc. with fac_lat, long
merge1 %>% select(fac_lat, fac_long, fac_accuracy_meters) %>% 
  summary(.)

# summary: at least ~zip code-level accuracy
merge1 %>% filter(fac_accuracy_meters <= 10000) %>% 
  count() %>% .$n
```

<br>

#### Next level up = city


```{r}
df_echo %>% filter(is.na(fac_zip), !is.na(fac_city))

```


+ Adding in merge2 city-associated values

```{r} 

merge2 <- df_echo %>% filter(is.na(fac_zip), !is.na(fac_city), sdwa_ids %in% merge1$PWSID) %>% 
  select(sdwa_ids, fac_lat, fac_long, fac_city, fac_county, fac_state, 
         fac_collection_method, fac_accuracy_meters) %>% 
  mutate(echo_dat = "fac_city used", 
         fac_lat = as.numeric(fac_lat), 
         fac_long = as.numeric(fac_long),
         fac_accuracy_meters = as.numeric(fac_accuracy_meters)) %>% 
  rename(PWSID = sdwa_ids)

setDT(merge1)[merge2, fac_lat := i.fac_lat, on =.(PWSID)]
merge1[merge2, fac_long := i.fac_long, on =.(PWSID)]
merge1[merge2, fac_city := i.fac_city, on =.(PWSID)]
merge1[merge2, fac_county := i.fac_county, on =.(PWSID)]
merge1[merge2, fac_state := i.fac_state, on =.(PWSID)]
merge1[merge2, fac_collection_method := i.fac_collection_method, on =.(PWSID)]
merge1[merge2, fac_accuracy_meters := i.fac_accuracy_meters, on =.(PWSID)]
merge1[merge2, echo_dat := i.echo_dat, on =.(PWSID)]


```

<br>

#### Next level up = County

```{r}
df_echo %>% filter(is.na(fac_zip), is.na(fac_city), !is.na(fac_county))

```


+ Adding in merge 3 county-associated values 

```{r} 

merge3 <- df_echo %>% filter(is.na(fac_zip), is.na(fac_city), !is.na(fac_county), 
                             sdwa_ids %in% merge1$PWSID) %>% 
  select(sdwa_ids, fac_lat, fac_long, fac_county, fac_state, 
         fac_collection_method, fac_accuracy_meters) %>% 
  mutate(echo_dat = "fac_county used", 
         fac_lat = as.numeric(fac_lat), 
         fac_long = as.numeric(fac_long),
         fac_accuracy_meters = as.numeric(fac_accuracy_meters)) %>% 
  rename(PWSID = sdwa_ids)

setDT(merge1)[merge3, fac_lat := i.fac_lat, on =.(PWSID)]
merge1[merge3, fac_long := i.fac_long, on =.(PWSID)]
merge1[merge3, fac_county := i.fac_county, on =.(PWSID)]
merge1[merge3, fac_state := i.fac_state, on =.(PWSID)]
merge1[merge3, fac_collection_method := i.fac_collection_method, on =.(PWSID)]
merge1[merge3, fac_accuracy_meters := i.fac_accuracy_meters, on =.(PWSID)]
merge1[merge3, echo_dat := i.echo_dat, on =.(PWSID)]


```


#### Next level up = State

+ adding all in, but should only use the few with higher accuracy

```{r}
df_echo %>% filter(is.na(fac_zip), is.na(fac_city), is.na(fac_county), !is.na(fac_state))

df_echo %>% filter(is.na(fac_zip), is.na(fac_city), is.na(fac_county), !is.na(fac_state)) %>% 
  .$fac_collection_method %>% table()

```

+ Adding in merge 4 state-associated values 

```{r} 

merge4 <- df_echo %>% filter(is.na(fac_zip), is.na(fac_city), is.na(fac_county), 
                             !is.na(fac_state),
                             sdwa_ids %in% merge1$PWSID) %>% 
  select(sdwa_ids, fac_lat, fac_long, fac_state, 
         fac_collection_method, fac_accuracy_meters) %>% 
  mutate(echo_dat = "fac_state used", 
         fac_lat = as.numeric(fac_lat), 
         fac_long = as.numeric(fac_long),
         fac_accuracy_meters = as.numeric(fac_accuracy_meters)) %>% 
  rename(PWSID = sdwa_ids)

setDT(merge1)[merge4, fac_lat := i.fac_lat, on =.(PWSID)]
merge1[merge4, fac_long := i.fac_long, on =.(PWSID)]
merge1[merge4, fac_state := i.fac_state, on =.(PWSID)]
merge1[merge4, fac_collection_method := i.fac_collection_method, on =.(PWSID)]
merge1[merge4, fac_accuracy_meters := i.fac_accuracy_meters, on =.(PWSID)]
merge1[merge4, echo_dat := i.echo_dat, on =.(PWSID)]

table(merge1$echo_dat)
```



+ most problematic ones, with PRIMACY_AGENCY_CODE not same as ADMIN address State, and no match to echo
```{r}
merge1 <- merge1 %>% 
  mutate(geo_filter = case_when(is.na(echo_dat) & PRIMACY_AGENCY_CODE != STATE_CODE ~ "NoMatch", 
         echo_dat == "fac_zip used" ~ "EchoZip",
         echo_dat == "fac_city used" ~ "EchoCity",
         echo_dat == "fac_county used" ~ "EchoCounty",
         echo_dat == "fac_state used" ~ "EchoState"))

table(merge1$geo_filter)
```

#### "best" ones so far
```{r}

merge1 %>% filter(!is.na(geo_filter), fac_accuracy_meters <= 10000) 

```


```{r}
# check just CWS in States
merge1 %>% filter(geo_filter == "EchoZip", PWS_TYPE_CODE == "CWS",
                  PRIMACY_AGENCY_CODE %in% c(state.abb))

```

+ so maybe ZIP_CODE5 not close to fac_zip? need to explore
```{r}
merge1 %>% filter(geo_filter == "EchoZip", PWS_TYPE_CODE == "CWS",
                  PRIMACY_AGENCY_CODE %in% c(state.abb), fac_zip != ZIP_CODE5)
```



## II. Join zip code data (to admin ZIP_CODE5, to check issue later)

<br>

+ zcta first

```{r}
merge1 <- left_join(merge1, zcta, by = c("ZIP_CODE5" = "GEOID"))

sum(is.na(merge1$INTPTLAT))
```


+ now zipcode for NA's 

```{r}
merge1 <- left_join(merge1, zipcode, by = c("ZIP_CODE5" = "zip"))

# create merged lat/lon columns and source column

merge1 <- merge1 %>% mutate(LAT = if_else(is.na(INTPTLAT), Rzcpkg_lat, INTPTLAT), 
                            LON = if_else(is.na(INTPTLONG), Rzcpkg_lon, INTPTLONG),
                            COORD_SRC = case_when(!is.na(INTPTLAT) ~ "zcta",
                                                  is.na(INTPTLAT) & !is.na(Rzcpkg_lat) ~ "rzcpkg"))


sum(is.na(merge1$LAT))

```

<br>

#### Count # w/o coordinates, by PWS_TYPE_CODE (row TRUE)

+ also # unique zipcodes, etc.

```{r}
length(unique(merge1$ZIP_CODE5))
table(is.na(merge1$LAT), merge1$PWS_TYPE_CODE)

table(is.na(merge1$fac_lat), merge1$PWS_TYPE_CODE)

length(unique(merge1$fac_zip))

```


#### Some checking, NY CWS:

```{r}

merge1 %>% filter(PWS_TYPE_CODE == "CWS", PRIMACY_AGENCY_CODE == "NY")

```



```{r}
table(df_echo$sdwa_system_types)

df_echo %>% filter(fac_state == "NY", fac_active_flag == "Y", 
                   sdwa_system_types == "Community water system")

```


#### Export dataset

+ adding city and state from zipcode package, may be useful later (also may be different than water_system)
```{r}
merge1 %>% select(PWSID:PWS_NAME, PWS_TYPE_CODE, PRIMACY_AGENCY_CODE,
                    ZIP_CODE5, LAT, LON, COORD_SRC, Rzcpkg_city:COORD_SRC, geo_filter) %>% 
  write.csv(gzfile("data_export/PWSID_coordinates.csv.gz"), row.names = FALSE)

```

<br>

#### Just making sure

```{r}
checkit <- readr::read_csv("data_export/PWSID_coordinates.csv.gz")

nrow(checkit)

head(checkit)

# number of NA's, the PWSIDs that were not matched
summary(checkit$fac_lat)
summary(checkit$fac_long)

# coordinate source when present
table(checkit$COORD_SRC)

table(checkit$geo_filter)

rm(checkit)

```

<br>

## III. Exploring things a bit (note holes since only fac_lat, fac_long, w/acc <= 10000)

<br>

this is nice:
https://eriqande.github.io/rep-res-web/lectures/making-maps-with-R.html

```{r fig.height=8, fig.width=8}

states <- map_data("state")

lwr48 <- state.abb[!state.abb %in% c("AK", "HI")]

# panel plot instead of together (some overlap if plot 1 on top of other)

merge1_CWS_48 <- merge1 %>% filter(STATE_CODE %in% c(lwr48, "DC"), 
                                   PWS_TYPE_CODE == "CWS",
                                   GW_SW_CODE %in% c("GW", "SW"),
                                   between(fac_long, -130, -60 ), 
                                   between(fac_lat, 25, 50 ),
                                   fac_accuracy_meters <= 10000)

ggplot(data = states) + 
  geom_polygon(aes(x = long, y = lat, group = group), fill = "white", color = "gray70") + 
  geom_point(data = merge1_CWS_48, 
             aes(x = fac_long, y = fac_lat, color = GW_SW_CODE), size = 0.8, alpha = 0.75) +
  coord_fixed(1.3) + 
  ggtitle("Zip codes with >= 1 Active CWS by surface and ground water as primary source") + 
  scale_color_manual(values = c("purple", "orange")) + facet_wrap(~GW_SW_CODE, ncol = 1)

```

<br><br><br><br>

# END

***

*In development*

<br>

#### Counting things at/within a distance of zip code coordinate

+ create spatial sp object

```{r}
library(sp)

Coords_CWS <- merge1_A %>% filter(PWS_TYPE_CODE == "CWS") %>% 
  dplyr::select(ZIP_CODE5, LAT, LON, STATE_CODE) %>% unique() %>% na.omit()

dups_zips <- which(duplicated(Coords_CWS$ZIP_CODE5))
dups_zips <- Coords_CWS[dups_zips, ]$ZIP_CODE5

Coords_CWS %>% filter(ZIP_CODE5 %in% dups_zips) %>% 
  arrange(ZIP_CODE5, STATE_CODE)

# so  probably easier to get state via spatial intersection (or drop these, some are potential errors)

# redo

Coords_CWS <- merge1_A %>% filter(PWS_TYPE_CODE == "CWS") %>% 
  dplyr::select(ZIP_CODE5, LAT, LON) %>% unique() %>% na.omit()

geo_prj <- "+proj=longlat +ellps=WGS84"

CWS_A_sp <- sp::SpatialPointsDataFrame(coords = Coords_CWS[,c("LON", "LAT")], 
                           data = Coords_CWS, proj4string = CRS(geo_prj))

plot(CWS_A_sp)
```

+ in this example, trimming to US 48 boundary

*found this for easy way to get US mainland:*

https://grokbase.com/t/r/r-sig-geo/106paa0c7w/how-to-transform-polygons-to-spatialpolygons

```{r}

library(maps)
library(maptools)
usa <- map("usa", plot = FALSE, fill = TRUE)
IDs <- sapply(strsplit(usa$names, ":"), function(x) x[1])
crs <- CRS("+proj=longlat +ellps=WGS84")
sp.usa <- map2SpatialPolygons(usa, IDs=usa$names, proj4string = crs)

mainland <- list(slot(sp.usa, "polygons")[[3]])
sp_mainland <- SpatialPolygons(mainland, proj4string = crs)
str(sp_mainland)
plot(sp_mainland)

```

*crop the zip codes:*

```{r}
CWS_A_sp_main <- CWS_A_sp[sp_mainland,] # easy way to crop!

plot(CWS_A_sp_main, cex = 0.2, col = "gray50")
```

+ get some environmental data (this can take some time and be quite large, here it is being downloaded to a folder created on a local drive; once done it will pull from there if re-run)
  
*following:*  
  
https://gis.stackexchange.com/questions/227585/how-to-use-r-to-extract-data-from-worldclim

```{r}
# worldclim also has future climate available at 2.5, 5, and 10 minutes of a degree; see ?getData

library(raster)

# can get big, so extracting to pre-defined local directory

r <- getData("worldclim", var = "bio", res = 2.5, path = "C:/temp/bioclim")
r

```

+ clean it up a bit and crop it

```{r}
rs <- r[[c(1, 5, 6, 7, 8, 12, 16, 17)]]
names(rs) <- c("TAM","TMAX", "TMIN", "TAR", 
               "TWQ", "PAN", "PWTQ", "PDQ")

# get extent and add a bit of a buffer
mainland_extent <- extent(sp_mainland) + c(-1, 1, -1, 1)

rsc <- crop(rs, mainland_extent)

# TAM (mean annual temp in C * 10)
plot(rsc$TAM)
```

+ get data for the zip codes (note: can be more involved, such as using a buffer; see ?extract)

```{r}
CWS_A_sp_main <- extract(x = rsc$TAM, y = CWS_A_sp_main, sp = TRUE)

head(CWS_A_sp_main)
nrow(CWS_A_sp_main)

# since data uses a scale factor of 10, to get degrees C:
summary(CWS_A_sp_main$TAM)/10

```

+ Make a map of it

```{r}
library(viridis) # some nice color schemes via ggplot2 added function scale_color_viridis

CWS_A_sp_main@data %>% 
  ggplot(aes(x = LON, y = LAT, color = TAM/10)) +
  geom_point(size = 0.8) + 
  scale_color_viridis() +
  ggtitle("Active, CWS zip codes: mean annual temperature (C)")

# saving sp object for use elsewhere (e.g., linking this environmental variable to PWSID)

saveRDS(CWS_A_sp_main, "data_export/Zips_CWS_A_sp_main_TAM.rds")

```

<br>

***

*In development*


#### linking to demographic info (at zcta-level)
https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=ACS_17_5YR_DP03&prodType=table

#### getting county
https://stackoverflow.com/questions/13316185/r-convert-zipcode-or-lat-long-to-county


