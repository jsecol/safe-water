---
title: "Map region 01 (New England) boundaries"
date: "`r Sys.Date()`"
output:
  github_document:
    toc: true
    toc_depth: 3
---

### Description

Brief description.

Data sources:

1. https://www.census.gov/geographies/mapping-files/time-series/geo/carto-boundary-file.html
2. https://www.census.gov/cgi-bin/geo/shapefiles/index.php?year=2018&layergroup=County+Subdivisions *(preferred)*
3. SELECTED ECONOMIC CHARACTERISTICS 2013-2017 American Community Survey 5-Year Estimates: 2017 latest year available at 

Coding outline:

1. 
2. 
3. 
4. 



### Load packages

```{r message=FALSE}

# # this package can work with googledrive to get a specific worksheet in a google sheet
# # (currently in development, not on CRAN)
# devtools::install_github("tidyverse/googlesheets4")

library(dplyr)
library(sf)
library(fs)
library(googledrive)
library(googlesheets4)
library(readr)
library(data.table)
library(lubridate)

```


### Unzip data (if needed)

+ if already have, can change `overwrite = FALSE`

```{r warning=FALSE}

zipdir1 <- dir_ls("C:/temp/CFB/vector/zipped", glob = "*.zip")

for(i in 1:length(zipdir1)) {
unzip(zipdir1[i],
      exdir = "C:/temp/CFB/vector/unzipped",
      junkpaths = TRUE,
      overwrite = FALSE)
}

# unzipped shapefile dir
shpdir <- dir_ls("C:/temp/CFB/vector/unzipped", glob = "*.shp") 

shpdir %>% 
  basename()

```

### Load shapefiles

```{r}

shp_list <- list()

for(i in 1:length(shpdir)) {
  shp <- st_read(shpdir[i], stringsAsFactors = FALSE, quiet = TRUE)
  shp <- st_transform(shp, crs = 26918)
  shp_list[[i]] <- shp
}

names(shp_list) <- basename(shpdir)

names(shp_list)

state_byfips <- c("CT", "ME", "MA", "NH", "RI", "VT")

names(shp_list) <- state_byfips

names(shp_list)

```

### Load locations data
*(from water shared drive, need to login/authenticate in web browser when run)*

```{r}

# # another way, but also can explore what's in the drive (includes any shared files)
# gdfiles <- drive_find(pattern = "Water", type = "spreadsheet")
# gdfiles$name
# gdfiles$id

locations_dat <- drive_get("WaterSystem_Locations") %>% 
  read_sheet(sheet = "Region01")

```

### Create spatial points

*same coordinate reference system as county subdivisions*

```{r}

st_crs(shp_list[[1]])

locations_pts <- st_as_sf(locations_dat, coords = c("LON", "LAT"), crs = 4163) %>% 
  st_transform(crs = st_crs(shp_list[[1]]))

```


### Combine them (join points state by state)

see: 
https://ryanpeek.github.io/mapping-in-R-workshop/vig_spatial_joins.html

```{r}

# Note: this skips PRIMACY_AGENCY_CODE "01", only 1 so will do them later

joined_list <- list()

for(i in 1:length(state_byfips)) {
  joined <- locations_pts %>% 
    filter(PRIMACY_AGENCY_CODE == state_byfips[i]) %>% 
    st_join(., left = TRUE, shp_list[[state_byfips[i]]]) %>% 
    st_drop_geometry()
  joined_list[[i]] <- joined
  
}

names(joined_list) <- state_byfips

joined_df <- do.call(bind_rows, joined_list)

# check if any not matched
sum(is.na(joined_df$INTPTLAT))

joined_df %>% filter(is.na(joined_df$INTPTLAT))

locations_dat %>% filter(PWSID == "MA2115001")

# 42.70396, -71.60161 appears to be bad, over border in NH

```

### View and export

```{r}

# put in github data folder eventually?

joined_df %>% 
  select(PRIMACY_AGENCY_CODE:PWSID, GEOID:NAME) %>% 
  head()

dir_create("C:/temp/CFB/data/processed")

joined_df %>% write_csv("C:/temp/CFB/data/processed/r01_pwsid_cosub.csv")

```

### Join to census data

*need better process/location for this, maybe combine first and add to Github data processed (along with spatial data?)*

```{r warning=FALSE}

zipdir2 <- dir_ls("C:/temp/CFB/table/zipped", glob = "*.zip")

for(i in 1:length(zipdir2)) {
unzip(zipdir2[i],
      exdir = paste0("C:/temp/CFB/table/unzipped", "/", "folder", i),
      junkpaths = TRUE,
      overwrite = FALSE)
}


census_files <- dir_ls("C:/temp/CFB/table/unzipped", 
                       recurse = TRUE, 
                       glob = "*ACS_17_5YR_DP03.csv")

census_list <- list()
for(i in 1:length(census_files)){
  dat <- read_csv(census_files[i], col_types = cols(.default = "c"))
  census_list[[i]] <- dat
}

census_dat <- do.call(bind_rows, census_list)

# lots of columns, trim down and convert to numeric where necessary

# HC03_VC161,Percent; PERCENTAGE OF FAMILIES AND PEOPLE WHOSE INCOME IN THE PAST 12 MONTHS IS BELOW THE POVERTY LEVEL - All families

census_dat1 <- census_dat %>% 
  select(GEO.id, GEO.id2, HC03_VC161) %>% 
  mutate(HC03_VC161 = as.numeric(HC03_VC161))

```

*a few missing*

```{r}
joined_df1 <- joined_df %>% 
  left_join(., census_dat1, by = c("GEOID" = "GEO.id2"))

summary(joined_df1$HC03_VC161)

```

```{r}

joined_df1 %>% filter(is.na(HC03_VC161))

```

***

### Example modeling use

+ Get SDWIS data

```{r}

base_dir <- "C:/temp"

path_SDWIS <- paste0(base_dir, "/SDWIS.zip")

SDWIS.zip_names <- grep('\\.csv$', unzip(path_SDWIS, list=TRUE)$Name, 
                           ignore.case=TRUE, value=TRUE)
SDWIS.zip_names

```

+ extraction to a common directory (overwrite = F: only if not already present)

```{r}

unzip(path_SDWIS, exdir = paste0(base_dir, "/sdwis_echo"), 
      files = SDWIS.zip_names[c(9, 7)],
      junkpaths = TRUE,
      overwrite = FALSE)

```

+ load (using data.table for speed)

```{r}
library(data.table)

# blank field detected, so fill = TRUE
water_system <- fread("C:/temp/sdwis_echo/WATER_SYSTEM.csv", 
                      sep = ",", 
                      colClasses=c("character"),
                      nThread = 1, 
                      fill = TRUE) %>% 
  as_tibble() %>% 
  filter(WATER_SYSTEM.EPA_REGION == "01", WATER_SYSTEM.PWS_ACTIVITY_CODE == "A")

viols <- fread("C:/temp/sdwis_echo/VIOLATION.csv", 
               sep = ",", 
               colClasses=c("character"),
               nThread = 1, 
               fill = TRUE) %>% 
  as_tibble() %>% 
  filter(VIOLATION.EPA_REGION == "01", VIOLATION.PWS_ACTIVITY_CODE == "A")


```

+ convert to date fields

```{r}

viols <- viols %>%
  mutate(VIOLATION.COMPL_PER_BEGIN_DATE = dmy(VIOLATION.COMPL_PER_BEGIN_DATE), 
         VIOLATION.COMPL_PER_END_DATE = dmy(VIOLATION.COMPL_PER_END_DATE), 
         VIOLATION.RTC_DATE = dmy(VIOLATION.RTC_DATE)) %>% 
  filter(TRUE)

```

+ get coliform data

```{r}

viol_coli <- viols %>% filter(VIOLATION.CONTAMINANT_CODE == "3100", 
                              VIOLATION.PWSID %in% locations_dat$PWSID)


table(viol_coli$VIOLATION.PRIMACY_AGENCY_CODE)

summary(viol_coli$VIOLATION.COMPL_PER_BEGIN_DATE)

```

+ reduce to counts and join all to location/cosub data

```{r}

viol_coli_n <- viol_coli %>% 
  filter(VIOLATION.IS_HEALTH_BASED_IND == "Y", 
         between(year(VIOLATION.COMPL_PER_BEGIN_DATE), 2013, 2017)) %>% 
  group_by(VIOLATION.PWSID) %>% 
  tally()

viol_coli_n

joined_df1 %>% 
  left_join(., viol_coli_n, by = c("PWSID" = "VIOLATION.PWSID")) %>% 
  mutate(n = tidyr::replace_na(n, 0))

# %>% tidyr::spread(primary_source_code, n)


```


1. get good pwsid sample

  + hurdle 1: how to identify pwsid has/doesn't have prior violation (if no prior, was it in existence?? - clean history issue: would love to be able to identify "clean" pwsid's say for 10 year prior, but don't know when pwsid founded. Seems possible that the first time a water system gets in the sdwis data is the first time it has any violation data, not how long it's been out there)
  
  + solution 1: only use pwsid if it has (any) data in violations over some time interval, produces a decent sample size that, e.g., based on modeling 2017 as outcome, has at least a 5-year prior history


```{r}

viols_all_5yrs_n <- viols %>% 
  filter(VIOLATION.PWSID %in% locations_dat$PWSID, 
         between(year(VIOLATION.COMPL_PER_BEGIN_DATE), 2013, 2017)) %>% 
  mutate(CPBD_YEAR = year(VIOLATION.COMPL_PER_BEGIN_DATE)) %>% 
  group_by(VIOLATION.PWSID, CPBD_YEAR) %>% 
  tally() %>% 
  mutate(n = 1) %>% 
  tidyr::spread(key = CPBD_YEAR, value = n, fill = 0, sep = "_")


viols_all_5yrs_n

```

+ sample for modeling has data in violations in 2013

```{r}

viols_all_5yrs_n %>% filter(CPBD_YEAR_2013 == 1)

```


+ Join the coliform HB back to this, and model

```{r}

modeldata <- viols_all_5yrs_n %>% filter(CPBD_YEAR_2013 == 1) 


modeldata <- viol_coli %>% 
  filter(VIOLATION.IS_HEALTH_BASED_IND == "Y", 
         between(year(VIOLATION.COMPL_PER_BEGIN_DATE), 2013, 2017)) %>% 
  select(VIOLATION.PWSID) %>% 
  distinct() %>% 
  mutate(COLI_HB_FLAG = 1) %>% 
  left_join(modeldata, ., by = "VIOLATION.PWSID") %>% 
  mutate(COLI_HB_FLAG = tidyr::replace_na(COLI_HB_FLAG, 0))


modeldata <- joined_df1 %>% 
  select(PWSID, PWS_TYPE_CODE, PRIMACY_AGENCY_CODE, 
         NAME, GEOID, HC03_VC161) %>% 
  left_join(modeldata, ., by = c("VIOLATION.PWSID" = "PWSID"))

modeldata

```

+ summary stats

```{r}

modeldata %>%
  group_by(PWS_TYPE_CODE, COLI_HB_FLAG) %>%
  tally() %>%
  tidyr::pivot_wider(names_from = COLI_HB_FLAG,
                     values_from = n,
                     names_prefix = "COLI_HB_FLAG_") %>%
  mutate(COLI_HB_prp = round(COLI_HB_FLAG_1 /
                               (COLI_HB_FLAG_0 + COLI_HB_FLAG_1), 2))
  
```


```{r}

modeldata %>%
  group_by(PRIMACY_AGENCY_CODE, COLI_HB_FLAG) %>%
  tally() %>%
  tidyr::pivot_wider(names_from = COLI_HB_FLAG,
                     values_from = n,
                     names_prefix = "COLI_HB_FLAG_") %>%
  mutate(COLI_HB_prp = round(COLI_HB_FLAG_1 /
                               (COLI_HB_FLAG_0 + COLI_HB_FLAG_1), 2))
  
```





















### Map them


```{r}
# just one, for example

plot(shp_list[[6]]$geometry)

plot(locations_pts[locations_pts$PRIMACY_AGENCY_CODE == "VT",]["PRIMACY_AGENCY_CODE"], 
     add = TRUE, col = "red")

```









